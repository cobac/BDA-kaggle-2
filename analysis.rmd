---
title: meh
output: html_document
---
<!-- The ipynb exporter doesn't like the yaml part of the file -->
# Physical activity recognition - Group 6
- David Coba
- Fridtjof Petersen 
- Leonhard Volz

# Setup

```{r}
library(tidyverse) 
library(e1071)
library(MASS)
library(caret)
```

```{r}
# Switch to TRUE when submitting the notebook to Kaggle
RUN_IN_KAGGLE <- FALSE
if (RUN_IN_KAGGLE) {
  # Copy all files to the current directory
  system("cp -r ../input/bda-2021-physical-activity-recognition/* ./")
} 
```

## Importing labels

```{r}
act_labels = read_table2("activity_labels.txt", col_names = FALSE)
act_labels = act_labels %>% dplyr::select(X1,X2)
act_labels 
```

```{r}
labels = read_delim("./RawData/Train/labels_train.txt", " ", col_names = F)
colnames(labels) <- c('trial', 'userid', 'activity', 'start', 'end')

labels = labels %>% mutate(activity = act_labels$X2[activity])
```

```{r}
print(labels)
```

```{r}
# Add the sequence start:end to each row in a list.
# The result is a nested table:
sample_labels_nested <- 
  labels %>% 
  rowwise() %>% # do next operation(s) rowwise
  mutate(sampleid = list(start:end)) %>%
  ungroup()

# Check the resulting table:
print(sample_labels_nested, n=6) 
```

```{r}
sample_labels <- 
  sample_labels_nested %>% 
# Rows are segments, we need to keep track of different segements
mutate(segment = row_number() ) %>% 
# Expand the data frame to one sample per row
unnest(cols = c(sampleid)) %>% 
# Remove columns we don't need anymore
dplyr::select(-c(start, end)) 

```

# Features

- Helper function to extract the time-series for the three dimensions of a sensor.

```{r}
extractSignals <- function(filename, sample_labels) {
  # extract user and experimental run ID's from file name
  username = gsub(".+user(\\d+).+", "\\1", filename) %>% as.numeric()
  expname  = gsub( ".+exp(\\d+).+", "\\1", filename) %>% as.numeric()
  
  # import the sensor signals from the file
  user <- read_delim(filename, " ", col_names = FALSE, progress = TRUE, 
                     col_types = "ddd")
  # merge signals with labels 
  user_df <- 
    data.frame(userid = username, trial = expname, user) %>%
    mutate(sampleid = 0:(nrow(user)-1) ) %>%
    left_join(sample_labels, by = c('userid','trial','sampleid'))  %>%
    # split in epochs of 128 samples 
    # one epoch = 2.56 sec
    mutate(epoch = sampleid %/% 128)
  
  return(user_df)
}
```

## Time-domain features

- We explore the histograms of the signals of a sample user.

```{r}
# extract accelerator signals
user1_df_acc <- extractSignals("./RawData/Train/acc_exp01_user01.txt", sample_labels)
# extract gyroscope signals
user1_df_gyr <- extractSignals("./RawData/Train/gyro_exp01_user01.txt", sample_labels)
```

```{r}
# histograms of accelerator scales
user1_df_acc %>%
  ggplot(aes(X1)) + 
  geom_histogram(bins=40, fill=1, alpha=0.5) + 
  geom_histogram(aes(X2), bins=40, fill = 2, alpha=0.5) + 
  geom_histogram(aes(X3), bins=40, fill = 4, alpha=0.5) +
  facet_wrap(~activity, scales = "free_y")
```

```{r}
# histograms of gyroscope scales
user1_df_gyr %>%
  ggplot(aes(X1)) + 
  geom_histogram(bins=40, fill=1, alpha=0.5) + 
  geom_histogram(aes(X2), bins=40, fill = 2, alpha=0.5) + 
  geom_histogram(aes(X3), bins=40, fill = 4, alpha=0.5) +
  facet_wrap(~activity, scales = "free_y")
```

- We see that histograms of different activities have different means, spread and shape.
- To capture these features we will use the means, standard deviations, skewness and kurtosis.

- Moreover, signals with different patterns have different autocorrelations. Slow signals tend to have high autocorrelations, while fast changing signals can have different values depending on their frequency and the sampling frequency.

```{r}
lagged_cor <- function(x, y=x, lag=0) {
  # compute correlation between x and a time shifted y
  r_lagged = cor(x, dplyr::lag(y, lag), use='pairwise')
  return(r_lagged)
}
```

- We also include the entropy of the time series. Entropy is a measure for the "information" or "surprisingness" of a distribution, so might very well be distinct between activities and thus make for a good predictor between activities.

```{r}
entropy  <- function(x, nbreaks = nclass.Sturges(x)) {
  r = range(x)
  x_binned = findInterval(x, seq(r[1], r[2], len= nbreaks))
  h = tabulate(x_binned, nbins = nbreaks) # fast histogram
  p = h/sum(h)
  -sum(p[p>0] * log(p[p>0]))
}
```

Lastly, a helper function to select the mode of a vector, taking into account NA values.

```{r}
most_common_value = function(x) {
  counts = table(x, useNA='no')
  most_frequent = which.max(counts)
  if (length(most_frequent) == 0) {
    return(NA)
  }else {
  return(names(most_frequent))
  }
}
```
## Frequency domain features

- Helper function that transforms a time-series dataframe into one of spectral densities.

```{r}
spectralDF <- function(df){
  # extract spectral bands for each time-series measure
  spectral_1 <- spectrum(df[, 1], plot = FALSE)
  spectral_2 <- spectrum(df[, 2], plot = FALSE)
  spectral_3 <- spectrum(df[, 3], plot = FALSE)
  # create output tibble of all three spectral bands
  out <- tibble(freq = spectral_1$freq,
                X1 = spectral_1$spec,
                X2 = spectral_2$spec,
                X3 = spectral_3$spec)
  return(out)
}
```

- These function take two inputs:
  - A vector with the different frequencies
  - A vector with the spectral densities
  
- Our first feature is the frequency with the highest density, the average frequency and the frequency variance. These features are characteristic of different patterns.

```{r}
# helper for maximum of spectral density
peak <- function(freq, spec){
  return(freq[which.max(spec)])
}

# helper to calculate the average frequency of a spectral density
freq_mean <- function(freq, spec){
  df   = freq[2] - freq[1]
  sbar = sum(freq * spec * df)
  return(sbar)  
}

# helper to calculate the variance of frequencies
freq_var <- function(freq, spec){
  df   = freq[2] - freq[1]
  xbar = sum(freq * spec * df)              # mean value
  svar = sum((freq - xbar)^2 * spec * df)   # centralised second moment
  return(svar)  
}
```

- Last, we include some features that analyze the shape of the frequency spectrum at different ranges

```{r}
createBandSummary <- function(freq, dimension, prefix){
  # Divide the frequency spectrum into 5 bands
  bands <- seq(0, max(freq), len = 5)
  specs <- data.frame(freq = freq, spec = dimension, prefix)
  bands_df <- specs %>% 
    # Find band to which freq belongs
    mutate(band = findInterval(freq, bands)) %>% 
    group_by(band) %>%
    # Extract the highest frequency and power per band
    summarise(band_peak_freq = freq[which.max(spec)],
            band_power = mean(spec)) %>%
    # Pivot so each column is a feature
    pivot_wider(id_cols = band,
                names_from = band,
                values_from = c(band_peak_freq,band_power))
  colnames(bands_df) <- paste0(prefix, "_", colnames(bands_df))
  return(bands_df)
}
```

# Putting it all together

- Extract time-domain features from the time-series of a sensor.

```{r}
extractTimeDomainFeatures <- function(signals){
  time_df <- 
    signals %>% 
    group_by(userid, trial, epoch) %>%
    summarise(
      # Epoch's, activity labels, initial sample & no. of samples
      activity = as.factor(most_common_value(activity)),
      sampleid = sampleid[1],
      n_samples = n(),
      ## Descriptive Features
      # means
      m1 = mean(X1), 
      m2 = mean(X2),
      m3 = mean(X3),
      energy1 = mean(X1^2),
      energy2 = mean(X2^2),
      energy3 = mean(X3^2),
      #standard deviation
      sd1 = sd(X1), 
      sd2 = sd(X2), 
      sd3 = sd(X3), 
      # skew
      skew1 = e1071::skewness(X1),
      skew2 = e1071::skewness(X2),
      skew3 = e1071::skewness(X3),
      # kurtosis
      kurt1 = e1071::kurtosis(X1),
      kurt2 = e1071::kurtosis(X2),
      kurt3 = e1071::kurtosis(X3),
      # Entropy
      entropy1 = entropy(X1),
      entropy2 = entropy(X2),
      entropy3 = entropy(X3),
      
      ## Time-domain features - correlation & autoregression
      # correlation between series
      cor12 = cor(X1, X2, use = "pairwise"),
      cor23 = cor(X2, X3, use = "pairwise"),
      cor13 = cor(X1, X3, use = "pairwise"),
      # auto-/cross-correlation X1
      AR11 = lagged_cor(X1, lag=1),
      AR12 = lagged_cor(X1, X2, lag=1),
      AR13 = lagged_cor(X1, X3, lag=1),
      # auto-/cross-correlation X2
      AR21 = lagged_cor(X2, X1,lag=1),
      AR22 = lagged_cor(X2, lag=1),
      AR23 = lagged_cor(X2, X3, lag=1),
      # auto-/cross-correlation X3
      AR31 = lagged_cor(X3, X1, lag=1),
      AR32 = lagged_cor(X3, X2, lag=1),
      AR33 = lagged_cor(X3, lag=1)
    )  %>%
rename(user_id = userid, exp_id = trial) %>%
ungroup()

  return(time_df)
}
```

- Converts the time-series of a sensor intro a spectral band and extracts frequency-related features.

```{r}
extractFreqDomainFeatures <- function(signals){
  # Convert the time-series intro frequency spectrum
  freq_signals <- signals %>% 
    dplyr::select(-c(segment, sampleid)) %>%
    group_by(userid, trial, epoch) %>%
    mutate(activity = most_common_value(activity)) %>% 
    ungroup()    %>%
    nest(ts = c(X1, X2, X3)) %>% 
    mutate(ts = map(ts, spectralDF))  %>%
    unnest(cols = c(ts))
  
  
  # Extract features
  freq_df <- freq_signals %>% 
    group_by(userid, trial, epoch) %>%
    summarise(
      # Lables
      activity = as.factor(most_common_value(activity)),
      # Features
      peak_x1 = peak(freq, X1),
      peak_x2 = peak(freq, X2),
      peak_x3 = peak(freq, X3),
      freq_mean_x1 = freq_mean(freq, X1),
      freq_mean_x2 = freq_mean(freq, X2),
      freq_mean_x3 = freq_mean(freq, X3),
      freq_var_x1 = freq_var(freq, X1),
      freq_var_x2 = freq_var(freq, X2),
      freq_var_x3 = freq_var(freq, X3),
      bands_x1 = createBandSummary(freq,X1,"X1"),
      bands_x2 = createBandSummary(freq,X2,"X2"),
      bands_x3 = createBandSummary(freq,X3,"X3")
    ) %>%
    # Unpack frequency-band features
    unpack(cols = c(bands_x1, bands_x2, bands_x3)) %>% 
    ungroup() %>% 
    rename(user_id = userid, exp_id = trial)

  return(freq_df)
}
```

- This function calls all the previous functions and extracts features from a dataset ("Test" or "Train").

```{r}
extractFeatures <- function(dataset){
  # Get filenames per sensor
  filenames_acc <- dir(paste0("./RawData/", dataset), "^acc", full.names = TRUE)
  filenames_gyr <- dir(paste0("./RawData/", dataset), "^gyr", full.names = TRUE)

  # Extract time-series per sensor
  data_acc <- map_dfr(filenames_acc, extractSignals, sample_labels) 
  data_gyr <- map_dfr(filenames_gyr, extractSignals, sample_labels) 

  # Calculate time-domain features
  time_df <- left_join(extractTimeDomainFeatures(data_acc),
                       extractTimeDomainFeatures(data_gyr),
                       by = c("user_id", "exp_id", "epoch",
                              "activity", "sampleid", "n_samples"),
                       suffix = c("_acc", "_gyr"))

  # Calculate frequency-domain features
  freq_df <- left_join(extractFreqDomainFeatures(data_acc),
                       extractFreqDomainFeatures(data_gyr),
                       by = c("user_id", "exp_id", "epoch","activity"),
                       suffix = c("_acc", "_gyr"))
                     
  # Merge time_df and freq_df
  output_df <- left_join(time_df, freq_df,
                         by = c("user_id", "exp_id", "epoch", "activity"))

  return(output_df)
}
```


# Model fitting
## Setup

Now, we extract all the features for the training dataset with the feature extraction function above:

```{r}
analysis_df <- extractFeatures("Train")
```

- There are 41 out of 1352 epochs that are not 128 samples long, all from the unlabeled activity.
 
```{r}
analysis_df %>%
  filter(n_samples != 128) %>%
  count(activity)
```

```{r}
analysis_df %>%
  filter(activity == "-") %>%
  count(activity)
```

- We decide to remove them so that the features that are sensible to the length of the epoch are not biased. Since this only concerns few features overall, excluding them should not bias our model substantially.

```{r}
# remove samples that are not a full epoch
analysis_df <- analysis_df %>%
  filter(n_samples == 128) %>%

# Remove non-feature variables
  dplyr::select(-c(user_id, exp_id, epoch, sampleid, n_samples)) %>%

# Standardize numeric variables
  mutate_if(is.numeric, ~c(scale(.)))
```

- There are some variables without variance, we delete them as they do not add informational values.

```{r}
zero_var_is <- caret::nearZeroVar(analysis_df)
head(zero_var_is)

analysis_df <- analysis_df[,-zero_var_is]
```

- We delete redundant variables if they are highly correlated with others, because co-linear predictors increase the risk for overfitting over generating potential informational value.

```{r}
cor_var_is <- caret::findCorrelation(cor(use = "pairwise.complete.obs",analysis_df %>%
                                           dplyr::select(-activity))) + 1
print(cor_var_is)

analysis_df <- analysis_df[, -cor_var_is]
```

## Model choices

### Model 1 - Linear discriminant analysis

```{r}
model_lda <- MASS::lda(activity ~ ., data = analysis_df) 
```

### Model 2 - Naive Bayes classifier

```{r}
model_naive_bayes <- e1071::naiveBayes(activity ~ ., data = analysis_df) 
```

### Why not other models
- Since we have a substantial number of variables, we think K-nearest neighbours algorithms will not perform adequately.
- Similarly, a quadratic discriminant analysis classifier will need to fit a very large number of parameters. QDA might potentially be a good model for this scenario if we performed some sort of feature selection first.

## Model selection

K-fold cross-validation with k = 10 to select the best model:

```{r}
trcntr <- trainControl('cv', number = 10)
```
- We fit the LDA without the features based on frequency bands:
```{r}
# without bands
cv_lda <- caret::train(activity ~ ., data = analysis_df %>% dplyr::select(-contains("band")),
                       method="lda", trControl = trcntr,na.action=na.exclude)
cv_lda 
```
- and a Naive Bayes with frequency bands
```{r}
cv_naive_bayes <- caret::train(activity ~ ., data = analysis_df,
                               method="naive_bayes", trControl = trcntr)
cv_naive_bayes
```

- The linear discriminant classifier seems to perform better.
- Our final model is a linear discriminant model, but we select features step-wise using 10-fold CV accuracy as our selection criteria.
  - However, this model is only selecting three features and perfroms worse than the plain LDA.
  
Therefore, we adjust the step-wise process.

```{r}
 model_lda_stepped <- train(activity ~ ., data = analysis_df,
                            method = "stepLDA",
                            trControl = trcntr)
 model_lda_stepped$finalModel
```

- Penalized L1 LDA

```{r}
l1_lda <- caret::train(x = as.data.frame(analysis_df[-1]),
                       y = analysis_df$activity,method ="PenalizedLDA",
                       na.action = na.exclude)

l1_lda$results$Accuracy
```

- Model with power features
```{r}
cv_lda_band <- caret::train(activity ~ ., data = analysis_df,
                       method="lda", trControl = trcntr,na.action=na.exclude)
cv_lda $results$Accuracy
cv_lda_band$results$Accuracy
```

## Visual Comparison of Accuracy

```{r}
set.seed(1)
inTraining <- createDataPartition(analysis_df$activity, p = .8, list = FALSE)
training <- analysis_df[ inTraining,]
testing  <- analysis_df[-inTraining,]

trcntr <- trainControl('cv', number = 10)

# Model without bands

cv_lda_no_bands <- train(x=training %>% dplyr::select(-activity,-contains("band")) %>% as.data.frame(),
                         y= training$activity,                      
                          method="lda", preProcess="knnImpute",
                         trControl = trcntr)

cv_lda_no_bands

pred_lda_no_bands <- predict(cv_lda_no_bands,testing)
res_lda_no_bands <-confusionMatrix(data = pred_lda_no_bands, reference = testing$activity)
res_lda_no_bands$overall

# Model with bands
cv_lda_bands <- train(x =as.data.frame(training[-1]),
                         y= training$activity,                      
                          method="lda", preProcess="knnImpute",
                         trControl = trcntr)

cv_lda_bands

pred_lda_bands <- predict(cv_lda_bands,testing)
res_lda_bands <-confusionMatrix(data = pred_lda_bands, reference = testing$activity)
res_lda_bands$overall


# model with bands peanalised
# deactivated as it breaks processing

# library for PenalizedLDA
#library(plyr)
#library(dplyr)
#cv_lda_bands_l1 <- train(x =as.data.frame(training[-1]),
#                         y= training$activity,                      
#                          method="PenalizedLDA", preProcess="knnImpute",
#                         trControl = trcntr)
#
#pred_lda_bands_l1 <- predict(cv_lda_bands_l1,testing)
#res_lda_bands_l1 <-confusionMatrix(data = pred_lda_bands_l1, reference = testing$activity)
#res_lda_bands_l1$overall


# knn without bands
cv_knn <- train(x=training %>% dplyr::select(-activity,-contains("band")) %>% as.data.frame(),
                         y= training$activity,                      
                          method="knn", preProcess="knnImpute",
                         trControl = trcntr)
cv_knn
pred_knn <- predict(cv_knn,testing)
res_knn <- confusionMatrix(data = pred_knn, reference = testing$activity)
res_knn$overall
# knn with bands
cv_knn_bands <- train(x=as.data.frame(training[-1]),
                         y= training$activity,                      
                          method="knn", preProcess="knnImpute",
                         trControl = trcntr)
cv_knn_bands
pred_knn_bands <- predict(cv_knn_bands,testing)
res_knn_bands <- confusionMatrix(data = pred_knn_bands, reference = testing$activity)


res_list <- list(lda_no_b=res_lda_no_bands$overall,
                 lda_b= res_lda_bands$overall,
                 knn= res_knn$overall,
                 knn_bands = res_knn_bands$overall)

sapply(res_list,'[',1)



```

# Submissions

```{r}
testing_df <- extractFeatures("Test")
testing_df$activity <- as.factor(testing_df$activity)
# Standardize features
testing_df <- testing_df %>% 
  # Only standardize features
  mutate_at(7:ncol(testing_df) , ~c(scale(.)))

test_activities <- predict(cv_lda_no_bands, testing_df)

testing_df <- testing_df %>%
  mutate(activity = test_activities) 
```

## Formatting the submission file

```{r}
output <- testing_df %>%
  # prepend "user" and "exp" to user_id and exp_id
  mutate(
    user_id = paste(ifelse(user_id < 10, "user0", "user"), user_id, sep=""), 
    exp_id = paste(ifelse(exp_id < 10, "exp0", "exp"), exp_id, sep="")
  ) %>% 
  # unit columnes user_id, exp_id and sample_id into a string 
  # separated by "_" and store it in the new variable `Id`
  unite(Id, user_id, exp_id, sampleid) %>%
  # retain only the `Id` and  predictions
  dplyr::select(Id, Predicted = activity) %>%
  # write to file
  write_csv("test_set_predictions.csv")

head(output)
```

# Division of labour

- Notebook Setup: Coba
- Feature extraction
    - Time-domain: Leonhard
    - Spectral features: Fridtjof
- Model selection: Coba, Leonhard
- Cross-validation: Coba, Fridtjof
