---
title: meh
output: html_document
---
<!-- The ipynb exporter doesn't like the yaml part of the file -->
# Physical activity recognition - Group 6
- David Coba
- Fridtjof Petersen 
- Leonhard Volz

# Setup

```{r}
library(tidyverse) 
library(e1071)
library(MASS)
library(caret)
```

```{r}
# Switch to TRUE when submitting the notebook to Kaggle
RUN_IN_KAGGLE <- FALSE
if (RUN_IN_KAGGLE) {
  # Copy all files to the current directory
  system("cp -r ../input/bda-2021-physical-activity-recognition/* ./")
} 
```


## Importing labels

```{r}
act_labels = read_table2("activity_labels.txt", col_names = FALSE)
act_labels = act_labels %>% dplyr::select(X1,X2)
act_labels 
```

```{r}
labels = read_delim("./RawData/Train/labels_train.txt", " ", col_names = F)
colnames(labels) <- c('trial', 'userid', 'activity', 'start', 'end')

labels = labels %>% mutate(activity = act_labels$X2[activity])
```

```{r}
print(labels)
```

```{r}
# Add the sequence start:end to each row in a list.
# The result is a nested table:
sample_labels_nested <- 
  labels %>% 
  rowwise() %>% # do next operation(s) rowwise
  mutate(sampleid = list(start:end)) %>%
  ungroup()

# Check the resulting table:
print(sample_labels_nested, n=6) 
```

```{r}
sample_labels <- 
  sample_labels_nested %>% 
# Rows are segments, we need to keep track of different segements
mutate(segment = row_number() ) %>% 
# Expand the data frame to one sample per row
unnest(cols = c(sampleid)) %>% 
# Remove columns we don't need anymore
dplyr::select(-c(start, end)) 

```

# Features


## Time-domain features

- We explore the histograms of the signals of a sample user

```{r}
user1_df_acc <- extractSignals("./RawData/Train/acc_exp01_user01.txt", sample_labels)
user1_df_gyr <- extractSignals("./RawData/Train/gyro_exp01_user01.txt", sample_labels)
```

```{r}
user1_df_acc %>%
  ggplot(aes(X1)) + 
  geom_histogram(bins=40, fill=1, alpha=0.5) + 
  geom_histogram(aes(X2), bins=40, fill = 2, alpha=0.5) + 
  geom_histogram(aes(X3), bins=40, fill = 4, alpha=0.5) +
  facet_wrap(~activity, scales = "free_y")
```

```{r}
user1_df_gyr %>%
  ggplot(aes(X1)) + 
  geom_histogram(bins=40, fill=1, alpha=0.5) + 
  geom_histogram(aes(X2), bins=40, fill = 2, alpha=0.5) + 
  geom_histogram(aes(X3), bins=40, fill = 4, alpha=0.5) +
  facet_wrap(~activity, scales = "free_y")
```

- We see that histograms of different activities have different means, spread and shape.
- To capture these features we will use the means, standard deviations, skewness and kurtosis.

- Moreover, signals with different patterns have different autocorrelations. Slow signals tend to have high autocorrelations, while fast changing signals can have different values depending on their frequency and the sampling frequency.

```{r}
lagged_cor <- function(x, y=x, lag=0) {
  # compute correlation between x and a time shifted y
  r_lagged = cor(x, dplyr::lag(y, lag), use='pairwise')
  return(r_lagged)
}
```

- We also include the entropy of the time series

```{r}
entropy  <- function(x, nbreaks = nclass.Sturges(x)) {
  r = range(x)
  x_binned = findInterval(x, seq(r[1], r[2], len= nbreaks))
  h = tabulate(x_binned, nbins = nbreaks) # fast histogram
  p = h/sum(h)
  -sum(p[p>0] * log(p[p>0]))
}
```

## Frequency domain features

- Helper function that tansforms a time-series df into one of spectral densities

```{r}
spectralDF <- function(df){
  spectral_1 <- spectrum(df[, 1], plot = FALSE)
  spectral_2 <- spectrum(df[, 2], plot = FALSE)
  spectral_3 <- spectrum(df[, 3], plot = FALSE)
  out <- tibble(freq = spectral_1$freq,
                X1 = spectral_1$spec,
                X2 = spectral_2$spec,
                X3 = spectral_3$spec)
  return(out)
}
```

- TODO: Exploratory plots of frequency bands per activity

- These function take two inputs:
- A vector with the different frequencies
- A vector with the spectral densities
  
- Our first feature is the frequency with the highest density, the average frequency and the frequency variance. These features are characteristic of different patterns.

```{r}
peak <- function(freq, spec){
  return(freq[which.max(spec)])
}
```

```{r}
freq_mean <- function(freq, spec){
  df   = freq[2] - freq[1]
  sbar = sum(freq * spec * df)               # 
  return(sbar)  
}
```

```{r}
freq_var <- function(freq, spec){
  df   = freq[2] - freq[1]
  xbar = sum(freq * spec * df)  
  svar = sum((freq - xbar)^2 * spec * df)
  return(svar)  
}
```

- TODO: Features related to the energy of different spectral bands
# Putting it all together

```{r}
# Helper function to select the mode of a vector, taking into account NA values
most_common_value = function(x) {
  counts = table(x, useNA='no')
  most_frequent = which.max(counts)
  if (length(most_frequent) == 0) {
    return(NA)
  }else {
  return(names(most_frequent))
  }
}
```

- Extract the time-series for the three dimensions of a sensor

```{r}
extractSignals <- function(filename, sample_labels) {
  # extract user and experimental run ID's from file name
  username = gsub(".+user(\\d+).+", "\\1", filename) %>% as.numeric()
  expname  = gsub( ".+exp(\\d+).+", "\\1", filename) %>% as.numeric()
  
  # import the sensor signals from the file
  user <- read_delim(filename, " ", col_names = FALSE, progress = TRUE, 
                     col_types = "ddd")
  # merge signals with labels 
  user_df <- 
    data.frame(userid = username, trial = expname, user) %>%
    mutate(sampleid = 0:(nrow(user)-1) ) %>%
    left_join(sample_labels, by = c('userid','trial','sampleid'))  %>%
    # split in epochs of 128 samples 
    # one epoch = 2.56 sec
    mutate(epoch = sampleid %/% 128)
  
  return(user_df)
}
```

- Extract time-domain features from the time-series of a sensor

```{r}
extractTimeDomainFeatures <- function(signals){
  time_df <- 
    signals %>% 
    group_by(userid, trial, epoch) %>%
    summarise(
      # Epoch's, activity labels, initial sample & no. of samples
      activity = as.factor(most_common_value(c("-", activity))),
      sampleid = sampleid[1],
      n_samples = n(),
      # Features
      # means
      m1 = mean(X1), 
      m2 = mean(X2),
      m3 = mean(X3),
      energy1 = mean(X1^2),
      energy2 = mean(X2^2),
      energy3 = mean(X3^2),
      #standard deviation
      sd1 = sd(X1), 
      sd2 = sd(X2), 
      sd3 = sd(X3), 
      q1_25 = quantile(X1, .25),
      skew1 = e1071::skewness(X1),
      # skew
      skew1 = e1071::skewness(X1),
      skew2 = e1071::skewness(X2),
      skew3 = e1071::skewness(X3),
      kurt1 = kurtosis(X1),
      kurt2 = kurtosis(X2),
      kurt3 = kurtosis(X3),
      # correlation between series
      cor12 = cor(X1, X2, use = "pairwise"),
      cor23 = cor(X2, X3, use = "pairwise"),
      cor13 = cor(X1, X3, use = "pairwise"),
      # auto-/cross-correlation X1
      AR11 = lagged_cor(X1, lag=1),
      AR12 = lagged_cor(X1, X2, lag=1),
      AR13 = lagged_cor(X1, X3, lag=1),
      # auto-/cross-correlation X2
      AR21 = lagged_cor(X2, X1,lag=1),
      AR22 = lagged_cor(X2, lag=1),
      AR23 = lagged_cor(X2, X3, lag=1),
      # auto-/cross-correlation X3
      AR31 = lagged_cor(X3, X1, lag=1),
      AR32 = lagged_cor(X3, X2, lag=1),
      AR33 = lagged_cor(X3, lag=1),
      # Entropy
      entropy1 = entropy(X1),
      entropy2 = entropy(X2),
      entropy3 = entropy(X3)
    )  %>%
rename(user_id = userid, exp_id = trial) %>%
ungroup()

  return(time_df)
}
```

- Converts the time-series of a sensor intro a spectral band and extracts frequency-related features

```{r}
extractFreqDomainFeatures <- function(signals){
  freq_signals <- signals %>% 
    dplyr::select(-c(segment, sampleid)) %>%
    group_by(userid, trial, epoch) %>%
    mutate(activity = most_common_value(activity)) %>% 
    ungroup()    %>%
    nest(ts = c(X1, X2, X3)) %>% 
    mutate(ts = map(ts, spectralDF))  %>%
    unnest(cols = c(ts))
  
  freq_df <- freq_signals %>% 
    group_by(userid, trial, epoch) %>%
    summarise(
      # Lables
      activity = as.factor(most_common_value(c("-", activity))),
      # Features
      peak_x1 = peak(freq, X1),
      peak_x2 = peak(freq, X2),
      peak_x3 = peak(freq, X3),
      freq_mean_x1 = freq_mean(freq, X1),
      freq_mean_x2 = freq_mean(freq, X2),
      freq_mean_x3 = freq_mean(freq, X3),
      freq_var_x1 = freq_var(freq, X1),
      freq_var_x2 = freq_var(freq, X2),
      freq_var_x3 = freq_var(freq, X3)
    ) %>%
    ungroup() %>% 
    rename(user_id = userid, exp_id = trial)

  return(freq_df)
}
```

- This function calls all the previous functions and extracts features from a dataset ("Test" or "Train")

```{r}
extractFeatures <- function(dataset){
  # Get filenames per sensor
  filenames_acc <- dir(paste0("./RawData/", dataset), "^acc", full.names = TRUE)
  filenames_gyr <- dir(paste0("./RawData/", dataset), "^gyr", full.names = TRUE)

  # Extract time-series per sensor
  data_acc <- map_dfr(filenames_acc, extractSignals, sample_labels) 
  data_gyr <- map_dfr(filenames_gyr, extractSignals, sample_labels) 

  # Calculate time-domain features
  time_df <- left_join(extractTimeDomainFeatures(data_acc),
                     extractTimeDomainFeatures(data_gyr),
                     by = c("user_id", "exp_id", "epoch",
                            "activity", "sampleid", "n_samples"),
                     suffix = c("_acc", "_gyr"))

# Calculate frequency-domain features
freq_df <- left_join(extractFreqDomainFeatures(data_acc),
                                         extractFreqDomainFeatures(data_gyr),
                                         by = c("user_id", "exp_id", "epoch","activity"),
                     suffix = c("_acc", "_gyr"))
                     
  # Merge time_df and freq_df
  output_df <- left_join(time_df, freq_df,
                         by = c("user_id", "exp_id", "epoch", "activity"))

  return(output_df)
}
```

```{r}
analysis_df <- extractFeatures("Train")
```

# Model fitting
## Setup

- Some epochs don't have assigned an activity, and there are some epoch that are not 128 samples long.
- However, we can see that both of these groups are the same.
- After removing all epochs without an activity assigned, all epochs are 128 samples long.

```{r}
analysis_df <- analysis_df %>%
  filter(activity != "-")
# Necessary to remove the "-" activity factor level
analysis_df$activity <- droplevels(analysis_df$activity)

analysis_df %>% 
  dplyr::select(n_samples) %>%
  table()
```


```{r}
# Remove non-feature variables
analysis_df <- analysis_df %>%
  dplyr::select(-c(user_id, exp_id, epoch, sampleid, n_samples))

# Standardize numeric variables
analysis_df <- analysis_df %>%
  mutate_if(is.numeric, ~c(scale(.)))
```

- There are no variables without variance

```{r}
zero_var_is <- caret::nearZeroVar(analysis_df)
head(zero_var_is)
```

- Delete redundant variables because they are highly correlated with others

```{r}
cor_var_is <- caret::findCorrelation(cor(analysis_df %>%
                                           dplyr::select(-activity))) + 1
print(cor_var_is)
```

```{r}
analysis_df <- analysis_df[, -cor_var_is]
```

## Model 1 - Linear discriminant analysis

```{r}
model_lda <- MASS::lda(activity ~ ., data = analysis_df) 
```

## Model 2 - Naive Bayes classifier

```{r}
model_naive_bayes <- e1071::naiveBayes(activity ~ ., data = analysis_df) 
```

## Why not other models

- Since we have a substantial number of variables, we think K-nearest neighbours algorithms will not perform adequately.
- Similarly, a quadratic discriminant analysis classifier will need to fit a very large number of parameters. QDA might potentially be a good model for this scenario if we performed some sort of feature selection first.

## Model selection

- K-fold cross-validation with k = 10 to select the best model

```{r}
trcntr <- trainControl('cv', number = 10)
```

```{r}
cv_lda <- caret::train(activity ~ ., data = analysis_df,
                       method="lda", trControl = trcntr)
cv_lda 
```

```{r}
cv_naive_bayes <- caret::train(activity ~ ., data = analysis_df,
                               method="naive_bayes", trControl = trcntr)
```

- The linear discriminant classifier seems to perform better.

# Submissions

```{r}
testing_df <- extractFeatures("Test")
testing_df$activity <- as.factor(testing_df$activity)
# Standardize features
testing_df <- testing_df %>% 
  # Only standardize features
  mutate_at(7:ncol(testing_df) , ~c(scale(.)))

test_activities <- predict(model_lda, testing_df)$class

testing_df <- testing_df %>%
  mutate(activity = test_activities) 
```


## Formatting the submission file

```{r}
output <- testing_df %>%
  # prepend "user" and "exp" to user_id and exp_id
  mutate(
    user_id = paste(ifelse(user_id < 10, "user0", "user"), user_id, sep=""), 
    exp_id = paste(ifelse(exp_id < 10, "exp0", "exp"), exp_id, sep="")
  ) %>% 
  # unit columnes user_id, exp_id and sample_id into a string 
  # separated by "_" and store it in the new variable `Id`
  unite(Id, user_id, exp_id, sampleid) %>%
  # retain only the `Id` and  predictions
  dplyr::select(Id, Predicted = activity) %>%
  # write to file
  write_csv("test_set_predictions.csv")

head(output)
```

# Division of labour

